{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M0_J6vT5qvGO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OiV7fEOPqv80"
   },
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Abjuqzgrq_Zw"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the above code, the text is tokenized, which means it is divided into individual words or tokens. The ‘Tokenizer’ object is created, which will handle the tokenization process. The ‘fit_on_texts’ method of the tokenizer is called, passing the ‘text’ as input. This method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index. The ‘total_words’ variable is then assigned the value of the length of the word index plus one, representing the total number of distinct words in the text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8GiYybRZrDXd"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the above code, the text data is split into lines using the ‘\\n’ character as a delimiter. For each line in the text, the ‘texts_to_sequences’ method of the tokenizer is used to convert the line into a sequence of numerical tokens based on the previously created vocabulary. The resulting token list is then iterated over using a for loop. For each iteration, a subsequence, or n-gram, of tokens is extracted, ranging from the beginning of the token list up to the current index ‘i’.\n",
    "\n",
    "*This n-gram sequence represents the input context, with the last token being the target or predicted word. This n-gram sequence is then appended to the ‘input_sequences’ list. This process is repeated for all lines in the text, generating multiple input-output sequences that will be used for training the next word prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fuNxoMufrTQ3"
   },
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the above code, the input sequences are padded to ensure all sequences have the same length. The variable ‘max_sequence_len’ is assigned the maximum length among all the input sequences. The ‘pad_sequences’ function is used to pad or truncate the input sequences to match this maximum length.\n",
    "\n",
    "*The ‘pad_sequences’ function takes the input_sequences list, sets the maximum length to ‘max_sequence_len’, and specifies that the padding should be added at the beginning of each sequence using the ‘padding=pre’ argument. Finally, the input sequences are converted into a numpy array to facilitate further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mhBJknX9rXK5"
   },
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the above code, the input sequences are split into two arrays, ‘X’ and ‘y’, to create the input and output for training the next word prediction model. The ‘X’ array is assigned the values of all rows in the ‘input_sequences’ array except for the last column. It means that ‘X’ contains all the tokens in each sequence except for the last one, representing the input context.\n",
    "\n",
    "*On the other hand, the ‘y’ array is assigned the values of the last column in the ‘input_sequences’ array, which represents the target or predicted word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HAfX_hsjraKU"
   },
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the above code, we are converting the output array into a suitable format for training a model, where each target word is represented as a binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "aaFMTFZAriv0",
    "outputId": "e950947f-61d6-449d-ba26-8bd5598312a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 17, 100)           820000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8200)              1238200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,208,800\n",
      "Trainable params: 2,208,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code above defines the model architecture for the next word prediction model. The ‘Sequential’ model is created, which represents a linear stack of layers. The first layer added to the model is the ‘Embedding’ layer, which is responsible for converting the input sequences into dense vectors of fixed size. It takes three arguments:\n",
    "\n",
    "**‘total_words’, which represents the total number of distinct words in the vocabulary; \n",
    "‘100’, which denotes the dimensionality of the word embeddings; \n",
    "and ‘input_length’, which specifies the length of the input sequences.\n",
    "The next layer added is the ‘LSTM’ layer, a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data. It has 150 units, which means it will learn 150 internal representations or memory cells.\n",
    "\n",
    "*Finally, the ‘Dense’ layer is added, which is a fully connected layer that produces the output predictions. It has ‘total_words’ units and uses the ‘softmax’ activation function to convert the predicted scores into probabilities, indicating the likelihood of each word being the next one in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bcGjQlBWrm-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3010/3010 [==============================] - 172s 55ms/step - loss: 6.2530 - accuracy: 0.0741\n",
      "Epoch 2/100\n",
      "3010/3010 [==============================] - 121s 40ms/step - loss: 5.5377 - accuracy: 0.1218\n",
      "Epoch 3/100\n",
      "3010/3010 [==============================] - 113s 38ms/step - loss: 5.1630 - accuracy: 0.1449\n",
      "Epoch 4/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 4.8440 - accuracy: 0.1635\n",
      "Epoch 5/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 4.5386 - accuracy: 0.1798\n",
      "Epoch 6/100\n",
      "3010/3010 [==============================] - 115s 38ms/step - loss: 4.2490 - accuracy: 0.2004\n",
      "Epoch 7/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 3.9718 - accuracy: 0.2246\n",
      "Epoch 8/100\n",
      "3010/3010 [==============================] - 126s 42ms/step - loss: 3.7095 - accuracy: 0.2540\n",
      "Epoch 9/100\n",
      "3010/3010 [==============================] - 132s 44ms/step - loss: 3.4606 - accuracy: 0.2880\n",
      "Epoch 10/100\n",
      "3010/3010 [==============================] - 123s 41ms/step - loss: 3.2307 - accuracy: 0.3225\n",
      "Epoch 11/100\n",
      "3010/3010 [==============================] - 119s 39ms/step - loss: 3.0171 - accuracy: 0.3580\n",
      "Epoch 12/100\n",
      "3010/3010 [==============================] - 121s 40ms/step - loss: 2.8193 - accuracy: 0.3925\n",
      "Epoch 13/100\n",
      "3010/3010 [==============================] - 122s 41ms/step - loss: 2.6375 - accuracy: 0.4265\n",
      "Epoch 14/100\n",
      "3010/3010 [==============================] - 134s 45ms/step - loss: 2.4692 - accuracy: 0.4597\n",
      "Epoch 15/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 2.3173 - accuracy: 0.4899\n",
      "Epoch 16/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 2.1760 - accuracy: 0.5178\n",
      "Epoch 17/100\n",
      "3010/3010 [==============================] - 121s 40ms/step - loss: 2.0467 - accuracy: 0.5449\n",
      "Epoch 18/100\n",
      "3010/3010 [==============================] - 121s 40ms/step - loss: 1.9259 - accuracy: 0.5696\n",
      "Epoch 19/100\n",
      "3010/3010 [==============================] - 119s 40ms/step - loss: 1.8204 - accuracy: 0.5932\n",
      "Epoch 20/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 1.7192 - accuracy: 0.6145\n",
      "Epoch 21/100\n",
      "3010/3010 [==============================] - 119s 39ms/step - loss: 1.6289 - accuracy: 0.6322\n",
      "Epoch 22/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 1.5435 - accuracy: 0.6525\n",
      "Epoch 23/100\n",
      "3010/3010 [==============================] - 121s 40ms/step - loss: 1.4674 - accuracy: 0.6689\n",
      "Epoch 24/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 1.3937 - accuracy: 0.6854\n",
      "Epoch 25/100\n",
      "3010/3010 [==============================] - 124s 41ms/step - loss: 1.3327 - accuracy: 0.6989\n",
      "Epoch 26/100\n",
      "3010/3010 [==============================] - 125s 42ms/step - loss: 1.2698 - accuracy: 0.7122\n",
      "Epoch 27/100\n",
      "3010/3010 [==============================] - 126s 42ms/step - loss: 1.2165 - accuracy: 0.7235\n",
      "Epoch 28/100\n",
      "3010/3010 [==============================] - 125s 41ms/step - loss: 1.1655 - accuracy: 0.7349\n",
      "Epoch 29/100\n",
      "3010/3010 [==============================] - 127s 42ms/step - loss: 1.1184 - accuracy: 0.7447\n",
      "Epoch 30/100\n",
      "3010/3010 [==============================] - 130s 43ms/step - loss: 1.0754 - accuracy: 0.7550\n",
      "Epoch 31/100\n",
      "3010/3010 [==============================] - 128s 42ms/step - loss: 1.0358 - accuracy: 0.7628\n",
      "Epoch 32/100\n",
      "3010/3010 [==============================] - 130s 43ms/step - loss: 1.0008 - accuracy: 0.7708\n",
      "Epoch 33/100\n",
      "3010/3010 [==============================] - 129s 43ms/step - loss: 0.9651 - accuracy: 0.7782\n",
      "Epoch 34/100\n",
      "3010/3010 [==============================] - 130s 43ms/step - loss: 0.9344 - accuracy: 0.7855\n",
      "Epoch 35/100\n",
      "3010/3010 [==============================] - 133s 44ms/step - loss: 0.9056 - accuracy: 0.7914\n",
      "Epoch 36/100\n",
      "3010/3010 [==============================] - 134s 44ms/step - loss: 0.8808 - accuracy: 0.7972\n",
      "Epoch 37/100\n",
      "3010/3010 [==============================] - 135s 45ms/step - loss: 0.8547 - accuracy: 0.8026\n",
      "Epoch 38/100\n",
      "3010/3010 [==============================] - 142s 47ms/step - loss: 0.8331 - accuracy: 0.8063\n",
      "Epoch 39/100\n",
      "3010/3010 [==============================] - 144s 48ms/step - loss: 0.8102 - accuracy: 0.8119\n",
      "Epoch 40/100\n",
      "3010/3010 [==============================] - 135s 45ms/step - loss: 0.7926 - accuracy: 0.8159\n",
      "Epoch 41/100\n",
      "3010/3010 [==============================] - 138s 46ms/step - loss: 0.7702 - accuracy: 0.8208\n",
      "Epoch 42/100\n",
      "3010/3010 [==============================] - 140s 47ms/step - loss: 0.7563 - accuracy: 0.8227\n",
      "Epoch 43/100\n",
      "3010/3010 [==============================] - 142s 47ms/step - loss: 0.7402 - accuracy: 0.8258\n",
      "Epoch 44/100\n",
      "3010/3010 [==============================] - 139s 46ms/step - loss: 0.7264 - accuracy: 0.8290\n",
      "Epoch 45/100\n",
      "3010/3010 [==============================] - 137s 46ms/step - loss: 0.7148 - accuracy: 0.8307\n",
      "Epoch 46/100\n",
      "3010/3010 [==============================] - 140s 47ms/step - loss: 0.7005 - accuracy: 0.8340\n",
      "Epoch 47/100\n",
      "3010/3010 [==============================] - 141s 47ms/step - loss: 0.6916 - accuracy: 0.8360\n",
      "Epoch 48/100\n",
      "3010/3010 [==============================] - 141s 47ms/step - loss: 0.6792 - accuracy: 0.8384\n",
      "Epoch 49/100\n",
      "3010/3010 [==============================] - 138s 46ms/step - loss: 0.6703 - accuracy: 0.8398\n",
      "Epoch 50/100\n",
      "3010/3010 [==============================] - 145s 48ms/step - loss: 0.6558 - accuracy: 0.8430\n",
      "Epoch 51/100\n",
      "3010/3010 [==============================] - 144s 48ms/step - loss: 0.6520 - accuracy: 0.8428\n",
      "Epoch 52/100\n",
      "3010/3010 [==============================] - 148s 49ms/step - loss: 0.6430 - accuracy: 0.8453\n",
      "Epoch 53/100\n",
      "3010/3010 [==============================] - 147s 49ms/step - loss: 0.6363 - accuracy: 0.8458\n",
      "Epoch 54/100\n",
      "3010/3010 [==============================] - 149s 49ms/step - loss: 0.6279 - accuracy: 0.8479\n",
      "Epoch 55/100\n",
      "3010/3010 [==============================] - 151s 50ms/step - loss: 0.6205 - accuracy: 0.8493\n",
      "Epoch 56/100\n",
      "3010/3010 [==============================] - 151s 50ms/step - loss: 0.6177 - accuracy: 0.8492\n",
      "Epoch 57/100\n",
      "3010/3010 [==============================] - 150s 50ms/step - loss: 0.6054 - accuracy: 0.8531\n",
      "Epoch 58/100\n",
      "3010/3010 [==============================] - 153s 51ms/step - loss: 0.6041 - accuracy: 0.8523\n",
      "Epoch 59/100\n",
      "3010/3010 [==============================] - 153s 51ms/step - loss: 0.5989 - accuracy: 0.8528\n",
      "Epoch 60/100\n",
      "3010/3010 [==============================] - 156s 52ms/step - loss: 0.5935 - accuracy: 0.8540\n",
      "Epoch 61/100\n",
      "3010/3010 [==============================] - 154s 51ms/step - loss: 0.5915 - accuracy: 0.8535\n",
      "Epoch 62/100\n",
      "3010/3010 [==============================] - 155s 51ms/step - loss: 0.5843 - accuracy: 0.8554\n",
      "Epoch 63/100\n",
      "3010/3010 [==============================] - 156s 52ms/step - loss: 0.5843 - accuracy: 0.8540\n",
      "Epoch 64/100\n",
      "3010/3010 [==============================] - 155s 52ms/step - loss: 0.5758 - accuracy: 0.8571\n",
      "Epoch 65/100\n",
      "3010/3010 [==============================] - 159s 53ms/step - loss: 0.5716 - accuracy: 0.8580\n",
      "Epoch 66/100\n",
      "3010/3010 [==============================] - 161s 54ms/step - loss: 0.5678 - accuracy: 0.8592\n",
      "Epoch 67/100\n",
      "3010/3010 [==============================] - 164s 55ms/step - loss: 0.5644 - accuracy: 0.8592\n",
      "Epoch 68/100\n",
      "3010/3010 [==============================] - 162s 54ms/step - loss: 0.5641 - accuracy: 0.8579\n",
      "Epoch 69/100\n",
      "3010/3010 [==============================] - 163s 54ms/step - loss: 0.5607 - accuracy: 0.8594\n",
      "Epoch 70/100\n",
      "3010/3010 [==============================] - 162s 54ms/step - loss: 0.5599 - accuracy: 0.8594\n",
      "Epoch 71/100\n",
      "3010/3010 [==============================] - 165s 55ms/step - loss: 0.5563 - accuracy: 0.8590\n",
      "Epoch 72/100\n",
      "3010/3010 [==============================] - 167s 55ms/step - loss: 0.5525 - accuracy: 0.8597\n",
      "Epoch 73/100\n",
      "3010/3010 [==============================] - 174s 58ms/step - loss: 0.5475 - accuracy: 0.8616\n",
      "Epoch 74/100\n",
      "3010/3010 [==============================] - 174s 58ms/step - loss: 0.5462 - accuracy: 0.8610\n",
      "Epoch 75/100\n",
      "3010/3010 [==============================] - 179s 59ms/step - loss: 0.5484 - accuracy: 0.8606\n",
      "Epoch 76/100\n",
      "3010/3010 [==============================] - 170s 57ms/step - loss: 0.5438 - accuracy: 0.8616\n",
      "Epoch 77/100\n",
      "3010/3010 [==============================] - 182s 60ms/step - loss: 0.5418 - accuracy: 0.8618\n",
      "Epoch 78/100\n",
      "3010/3010 [==============================] - 185s 61ms/step - loss: 0.5413 - accuracy: 0.8609\n",
      "Epoch 79/100\n",
      "3010/3010 [==============================] - 185s 61ms/step - loss: 0.5409 - accuracy: 0.8613\n",
      "Epoch 80/100\n",
      "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5334 - accuracy: 0.8638\n",
      "Epoch 81/100\n",
      "3010/3010 [==============================] - 184s 61ms/step - loss: 0.5326 - accuracy: 0.8635\n",
      "Epoch 82/100\n",
      "3010/3010 [==============================] - 188s 62ms/step - loss: 0.5349 - accuracy: 0.8626\n",
      "Epoch 83/100\n",
      "3010/3010 [==============================] - 199s 66ms/step - loss: 0.5303 - accuracy: 0.8636\n",
      "Epoch 84/100\n",
      "3010/3010 [==============================] - 201s 67ms/step - loss: 0.5314 - accuracy: 0.8630\n",
      "Epoch 85/100\n",
      "3010/3010 [==============================] - 204s 68ms/step - loss: 0.5268 - accuracy: 0.8640\n",
      "Epoch 86/100\n",
      "3010/3010 [==============================] - 223s 74ms/step - loss: 0.5283 - accuracy: 0.8631\n",
      "Epoch 87/100\n",
      "3010/3010 [==============================] - 230s 76ms/step - loss: 0.5282 - accuracy: 0.8623\n",
      "Epoch 88/100\n",
      "3010/3010 [==============================] - 236s 78ms/step - loss: 0.5262 - accuracy: 0.8632\n",
      "Epoch 89/100\n",
      "3010/3010 [==============================] - 227s 76ms/step - loss: 0.5234 - accuracy: 0.8643\n",
      "Epoch 90/100\n",
      "3010/3010 [==============================] - 257s 85ms/step - loss: 0.5265 - accuracy: 0.8627\n",
      "Epoch 91/100\n",
      "3010/3010 [==============================] - 243s 81ms/step - loss: 0.5237 - accuracy: 0.8645\n",
      "Epoch 92/100\n",
      "3010/3010 [==============================] - 220s 73ms/step - loss: 0.5206 - accuracy: 0.8648\n",
      "Epoch 93/100\n",
      "3010/3010 [==============================] - 247s 82ms/step - loss: 0.5189 - accuracy: 0.8650\n",
      "Epoch 94/100\n",
      "3010/3010 [==============================] - 276s 92ms/step - loss: 0.5238 - accuracy: 0.8632\n",
      "Epoch 95/100\n",
      "3010/3010 [==============================] - 258s 86ms/step - loss: 0.5193 - accuracy: 0.8645\n",
      "Epoch 96/100\n",
      "3010/3010 [==============================] - 258s 86ms/step - loss: 0.5169 - accuracy: 0.8637\n",
      "Epoch 97/100\n",
      "3010/3010 [==============================] - 311s 103ms/step - loss: 0.5150 - accuracy: 0.8653\n",
      "Epoch 98/100\n",
      "3010/3010 [==============================] - 267s 89ms/step - loss: 0.5197 - accuracy: 0.8635\n",
      "Epoch 99/100\n",
      "3010/3010 [==============================] - 267s 89ms/step - loss: 0.5158 - accuracy: 0.8647\n",
      "Epoch 100/100\n",
      "3010/3010 [==============================] - 314s 104ms/step - loss: 0.5119 - accuracy: 0.8663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d8eff85760>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the above code, the model is being compiled and trained. The ‘compile’ method configures the model for training. The ‘loss’ parameter is set to ‘categorical_crossentropy’, a commonly used loss function for multi-class classification problems. The ‘optimizer’ parameter is set to ‘adam’, an optimization algorithm that adapts the learning rate during training.\n",
    "The ‘metrics’ parameter is set to ‘accuracy’ to monitor the accuracy during training. After compiling the model, the ‘fit’ method is called to train the model on the input sequences ‘X’ and the corresponding output ‘y’. The ‘epochs’ parameter specifies the number of times the training process will iterate over the entire dataset. The ‘verbose’ parameter is set to ‘1’ to display the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GfeAC09WrsI0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "I will leave if they come to have been here we\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"I will leave if they\"\n",
    "next_words = 6\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
